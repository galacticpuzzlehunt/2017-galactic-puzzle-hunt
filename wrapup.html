


<!DOCTYPE html>

<html>
    <head>
        <meta charset="utf-8"><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(2),u=e(3),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}finally{f.emit("fn-end",[c.now()],t)}}}};a("setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now()])}},{}],2:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],3:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],4:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=m(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){v[e]=m(e).concat(n)}function m(e){return v[e]||[]}function w(e){return p[e]=p[e]||o(t)}function g(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var v={},y={},b={on:l,emit:t,get:w,listeners:m,context:n,buffer:g,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(2),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!x++){var e=h.info=NREUM.info,n=d.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+h.offset],null,"api");var t=d.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===d.readyState&&i()}function i(){f("mark",["domContent",a()+h.offset],null,"api")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-h.offset}var u=(new Date).getTime(),f=e("handle"),c=e(2),s=e("ee"),p=window,d=p.document,l="addEventListener",m="attachEvent",w=p.XMLHttpRequest,g=w&&w.prototype;NREUM.o={ST:setTimeout,CT:clearTimeout,XHR:w,REQ:p.Request,EV:p.Event,PR:p.Promise,MO:p.MutationObserver};var v=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1026.min.js"},b=w&&g&&g[l]&&!/CriOS/.test(navigator.userAgent),h=n.exports={offset:u,now:a,origin:v,features:{},xhrWrappable:b};e(1),d[l]?(d[l]("DOMContentLoaded",i,!1),p[l]("load",r,!1)):(d[m]("onreadystatechange",o),p[m]("onload",r)),f("mark",["firstbyte",u],null,"api");var x=0,E=e(4)},{}]},{},["loader"]);</script><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"beacon":"bam.nr-data.net","queueTime":0,"licenseKey":"0be415eea6","agent":"","transactionName":"ZgBRN0JSV0VZBxEMW19KdRZeUE1fVwpKFUFLH18GQx1PX10TFl9DQwRDFkA=","applicationID":"51841001","errorBeacon":"bam.nr-data.net","applicationTime":12}</script>
        <title>Galactic Puzzle Hunt</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" href="static/images/airplaneFavicon.png">
        <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:700i|Roboto:400,700" rel="stylesheet">
        <link rel="stylesheet" href="static/Skeleton-2.0.4/css/normalize.css">
        <link rel="stylesheet" href="static/Skeleton-2.0.4/css/skeleton.css">
        <link rel="stylesheet" href="static/css/base.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.css">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" crossorigin="anonymous"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script>
        <script type="text/javascript" src="static/js/jquery.formset.js"></script>
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    </head>
    <body>
        <div class="top-bar-container">
             <div class="nav-inner-container">
                <nav class="main-nav">
                    <a href='index.html'>Home</a>
                    <a href='rules.html'>Rules</a>
                    
                        <a href='story.html'>Story</a>
                    
                    <a href='puzzles.html'>Puzzles</a>
                    <a href='teams.html'>Teams</a>
                    <a href='faq.html'>FAQ</a>
                    
                        <a href='errata.html'>Updates</a>
                    
                    <a href='wrapup.html'>Wrap-Up</a>
                    <a href='https://galacticpuzzlehunt.com/archive'>Archive</a>
                </nav>
             </div>
        </div>
        <div class="center">
            <div class="row">
                <div class="six columns">
                
                </div>
                <div class="login-controls-container">
                
                </div>
            </div>
            

<style>
  .tight_margins * {
    margin-bottom: 0px;
    margin-top: 0px;
  }
</style>


<div class="single-narrower-column-on-desktop">
<h1 class="page-header">Wrap-Up</h1>

<p>Congratulations to the top 4 teams, <strong>Duck Soup</strong>, <strong>Brown Herrings</strong>, <strong>Pluru</strong>, and <strong>Synod</strong>, who finished each of the 30 puzzles within 24 hours! Also, congratulations to <strong>Test Solution, Please Ignore</strong> for being the first team to solve the final meta, as well as the first team to finish all 5 metas.</p>

<p>The rest of this page will be a recap of the process of making this hunt, followed by some fun stats, so as a warning, there will be lots of puzzle spoilers. It’s also very long, so feel free to just skip to the <a href="wrapup.html#fun">fun stuff</a>!</p>

<h2>Plot Summary</h2>

<p>On the first day of the hunt, teams received a “mysterious note” from an anonymous source who had valuable intelligence on the secret strategies of the teams competing in the annual Puzzleball Championships. These strategies were obfuscated with puzzles (of course), so the source needed the team’s help in order to discover the strategies and use them to fill out a perfect bracket.</p>

<p>On the sixth day of the hunt, teams received another email from this source, informing them that the Puzzleball teams had split themselves up into three “conferences” (metas) each of which had decided to follow some group strategy. Furthermore, 9 of the 25 teams were going to be disqualified for an undetermined reason.</p>

<p>Teams needed to solve the Duck Quonundrum meta to discover which 9 teams were disqualified, and then solve enough of the conference metas to realize that the “secret strategies” were simply Rock, Paper, or Scissors. With this knowledge teams could then fill out the final bracket (our metameta) and extract the final answer REF PASSED SECRETS, which answered the question “how did you get this information in the first place?”.</p>

<p>Upon solving the metameta, teams were emailed the following message, accompanied with a personalized certificate:</p>

<div class="no-break story-chunk">
  <p class="story-chunk">Dear $TEAMNAME,</p>

  <p>Thank you for informing us of the disturbing actions taken by one of our referees. The moment we received your phone call we launched a full-scale investigation into the matter and we quickly identified the individual responsible. Rest assured that he has been terminated. It appears that his goal was to steal confidential team documents in order to somehow predict the results of the Puzzleball tournament.</p>

  <p>As a result, we’ve decided to cancel this year’s “Perfect Bracket” contest. This is not a decision we made lightly, but we value the integrity of this contest too much to take any other action. While our official rules prevent us from awarding you even a small monetary token of appreciation for your valuable tip-off, we hope that the attached certificate below should be a worthy prize on its own.</p>

  <p>Sincerely,</br>
    Ark Memmert,</br>
    President of the PCAA (Puzzleball Championships Athletic Association)</p>
</div>

<img src="static/images/wrapup/certificate.png" style="max-width: 100%">

<h2>Writing the Hunt</h2>

<h4>Initial Conception</h4>

<p>The story begins with the 2017 MIT Mystery Hunt. Through a combination of an easier than normal hunt and a somewhat strengthened ✈✈✈ Galactic Trendsetters ✈✈✈, we finished about 24 hours after the hunt’s start. The shortened hunt left some of us with a desire to keep puzzling, so we decided to spend the next day or two constructing another round complete with a meta — what eventually became the Conference 3 meta.</p>

<p>We worked on it for most of Sunday, but in the end it was clear that additional polish was needed to release it publicly. So, we saved what puzzles we had and polled our team members on our next step forward. Enough people expressed interest in creating a full-fledged hunt that we decided to get organized and start planning.</p>

<h4>Theme and Hunt Structure</h4>

<p>We picked the date early on, in order to pin down a deadline to work towards. Pi Day was both thematic (the MIT dorm our mystery hunt team is based around is called Floor Pi) and approximately midway between Mystery Hunt and <a href="http://www.ms.unimelb.edu.au/~mums/puzzlehunt/2016/">MUMS</a>, so we chose this date and worked backwards from it to create a schedule we tried to stick to.</p>

<p>We also considered our meta structure at this time. We wanted to make metas a small surprise for teams, so a suggestion early on that made it into the hunt was to release 5 days of regular puzzles and a final day of only metapuzzles. We wanted to smoothly integrate the meta we’d already created and tested, so we spent some time thinking about what we could do with Conference 3’s answer, BUNNY EARS.</p>

<p>Someone mentioned that it was the same hand-sign as SCISSORS in Rock Paper Scissors, and this inspired us to create a metameta that involved filling out a bracket resolved by RPS games. This in turn motivated our hunt theme: March Madness. To our pleasant surprise, March Madness began on Pi Day this year, which certainly sealed the deal for us.</p>

<p>The next few days, we considered answers suitable for the metameta. After some deliberation, we settled on the phrase REF PASSED SECRETS, since it succinctly captured the RPS theme and contained exactly 16 letters. In the meantime, we already had proposals for our Conference 1 (rock) and Conference 2 (paper) metas. It was fairly unconventional building the metameta around the metas, as opposed to the other way around, but in the end we were able to finagle the answers into the 16-team bracket with the desired extraction mechanism.</p>

<p>Finally, we now had 25 - 16 = 9 answers that weren’t used at all in our metameta. We considered simply telling teams which 9 teams were "disqualified", but we decided it would be nice to add another metapuzzle in which teams discovered this information for themselves. (This also preserved the nice property of releasing 5 puzzles every day).</p>

<p>We therefore had to write a metapuzzle in which all the feeder answers were pre-determined, but the final answer was basically unconstrained. A conundrum-style meta provided plenty of freedom within our constraints, and this eventually became the Duck Quonundrum meta.</p>

<h4>Puzzle Writing Process</h4>
<p>Over the course of the six weeks leading up to this hunt, each puzzle was written, testsolved and revised (often several times), and then fact-checked. Our website was built from scratch with Django, and each puzzle and its solution was converted into HTML.</p>
<p>Most puzzles went through several iterations of revision and testsolving before making it into our hunt. We also scrapped a few puzzles in various stages of creation, including two other video game puzzles (we felt we had too many already) and another interactive game, which we thought was unfair to teams without programmers (if you’re interested, you can try out the puzzle <a href="http://beta.vero.site/two-button-idle-game">here</a>).</p>

<h4>Puzzle Ordering</h4>
<p>Our intention in puzzle ordering was, like the Australian hunts, to gradually dial up the difficulty over the course of the hunt, as well as to order the puzzles by difficulty within a day. (We were only partially successful on this point — for example, Zero Space and Thunk! having so many solves was a surprise.) We also tried to ensure that each day had at least one easier puzzle, and that the first day had all fairly accessible puzzles.</p>
<p>We knew that Scramble for the Stars was likely to be the hardest regular puzzle in our hunt, but we intentionally put it on Day 3 because it is particularly backsolvable from both the DQ and Conference 2 metas (and perhaps even backsolvable with only the knowledge that some ingredients were substrings prior to Day 6). We didn’t want teams to be able to backsolve it without incurring a fairly large AAST penalty, to reward teams who solved it without meta knowledge. That said, we’re not inherently opposed to backsolving, and it was fully intended that teams could backsolve some of the harder puzzles once the meta structure was revealed.</p>


<h4>Organization</h4>
<p>We did almost all of our organization on a Discord server, with specific channels for everything from web design, to testsolving requests, to puzzle idea discussion. (We're still throwing around new ideas for puzzles in that last one!) To monitor the status of the hunt, we used webhooks to automatically inform us of team registrations, hint requests, and answer submissions.</p>
<p>We really enjoyed watching teams solve puzzles and submit answers in real-time:</p>

<img src="static/images/wrapup/discord01.png" style="max-width: 100%">
<a href="static/images/wrapup/discord02.png">
<img src="static/images/wrapup/discord02.png" style="max-width: 100%">
</a>

<h2>What We Learned</h2>
<h4 id="hint-system">Hint System</h4>
<p>Our original intent in the hints and scoring is outlined in our FAQ. We spent a lot of time debating different alternatives, but basically the only thing we all agreed on is that it is impossible to make something that is fair to everyone. In the end, we chose a system that we hoped would be a good compromise, and now that the hunt is over, we have a number of thoughts about the result:</p>
<ul>
  <li>Yes/No hints are helpful if you’ve made some progress and want to confirm or disprove possible solving paths, but if you’re just completely stuck and have no leads, then these hints aren’t as useful.</li>
  <li>We eventually decided to give teams infinite hints, but this soon became a major hassle because of the large volume. It became especially unwieldy in combination with non-Y/N questions, even though the change did let us help teams more effectively.</li>
</ul>

<p>If we run this again in the future, we will likely use a different system. We're currently considering a hybrid system of hinting that might involve both Y/N questions and pre-written hints.</p>

<p>We also have some fun stats about the hint system:</p>

<img src="static/images/wrapup/hint_response.png" style="max-width: 100%">

<p>We answered <b>2187</b> hints over the course of the hunt. Unsurprisingly, most of these hints were asked after we eventually removed the hint limit. Our median response time was <b>3.45 minutes</b>, and our average response time was <b>8.2 minutes</b>.</p>

<img src="static/images/wrapup/hint_puzzle.png" style="max-width: 100%">

<p>Scramble for the Stars was the most asked about puzzle, with <b>160</b> hints being asked. Duck Quonundrum was the least asked about puzzle, with <b>12</b> hints being asked.</p>

<img src="static/images/wrapup/hint_queue.png" style="max-width: 100%">
<p>Our average hint queue size was 1.534 unanswered hints. We had zero hints in the queue 53.1% of the time, and one hint in the queue 20.6% of the time. At peak, we had 38 hints in the queue (on the final evening of the hunt).</p>

<h4>Scoring System</h4>
<p>Even though our traffic statistics showed that most puzzlers were based in US timezones, many puzzlers appreciated that our scoring policy did not penalize teams as long as they solved within 24 hours, both for work schedule reasons and our overseas friends in other countries.</p>
<p>Nonetheless, our nonstandard scoring scheme introduced other issues, such as:</p>
<ul>
  <li>The scoring system incentivizes teams to work on puzzles which have been open for more than 24 hours (which were likely puzzles they were already frustrated with) instead of looking at new puzzles.</li>
  <li>There’s a substantial scoring difference between solving a puzzle in less than 24 hours and solving it afterwards, even though the puzzle doesn’t get any easier after a day (unlike in other Australian hunts where a hint is automatically released then).</li>
</ul>

<p>We already have some ideas for how to get around these issues, but it’s a tough problem which we’ll continue to think about.</p>

<h4>Hunt Format / Difficulty</h4>

<p>Many teams enjoyed the moderate, spread-out pace at which puzzles were released, but we also heard (and felt) that the hunt went on for too long. Most of our team was exhausted after 8 days of answering hints and responding to emails. In retrospect, we would probably make our hunt shorter. However, many people aren't willing to spend an entire weekend working hard on our puzzles; a four- or five-day hunt ending on a Sunday might give a good balance between these two preferences.</p>
<p>According to our feedback form, 48% of people agreed with the statement "I wanted more easy puzzles in the hunt". 58% of people agreed with "I wanted more medium puzzles", and only 7% of people agreed with "I wanted more hard puzzles" (while 63% disagreed). Clearly, many teams were not satisfied with the number of approachable puzzles in the hunt. However, we received extremely favorable responses for the questions "The puzzles in this hunt were fun" and "The puzzles in this hunt were creative" (87% and 93% agreed, respectively) so we're somewhat hesitant to think that a large shift towards easy puzzles is the right thing to do.</p>
<p>We feel that this problem can be best addressed through format. Hard puzzles aren't for every team, but this hunt's format wasn't great at guiding teams to the puzzles that were appropriate for them. Many teams told us that they were thrilled to finally get past a step of a puzzle, only to find that they had to do another layer to get to the answer, which could be demotivating.</p>
<p>Two ideas that may improve these teams' experiences are moving closer towards a Mystery Hunt-style of unlocks (where harder puzzles are "gated" behind easier ones), and difficulty ratings for each puzzle.</p>

<h2 id='fun'>Fun Stuff</h2>

<p>Here’s some fun items we created and compiled over the course of the hunt:</p>

<ul class="tight_margins">
  <li>A <a href="amusing.html">list</a> of wrong answer submissions we liked</li>
  <li><a href="lyrics.html">Lyrics</a> to "Still Alive" made up of (mostly) answer submissions to Angry Portals</li>
  <li>A <a href="https://play.spotify.com/user/12120178570/playlist/26jhyWlgEH6MeSI73eGv7m">Spotify playlist</a> containing some songs that might be stuck in your head right now</li>
  <li>A complete <a href="static/guess_log.csv">guess log</a> for the entire hunt (if you come up with any interesting statistics, please share them with us!)</li>
  <li><a href="bigboard.html">Big Board</a> (the page we used to monitor team's global progress)</li>
</ul>

<h2>Statistics</h2>

<p>Here are some fun statistics:</p>

<ul class="tight_margins">
    <li>19 teams achieved a full score</li>
    <li>27 teams solved the metameta</li>
    <li>282 teams solved at least one puzzle</li>
    <li>278 teams solved the easiest puzzle, Puzzle of the Day</li>
    <li>23 teams solved the hardest puzzle, Conference 1</li>
    <li>40 teams solved the hardest non-meta, Scramble for the Stars</li>
    <li>2187 hints were asked</li>
    <li>Most common wrong answers:
        <ul>
            <li>128 teams guessed 'HUGESUCCESS' on Angry Portals</li>
            <li>106 teams guessed 'STILLALIVE' on Angry Portals (and one guessed it on A Glistening Occasion)</li>
            <li>84 teams guessed 'APPLE' on Famous by Association</li>
            <li>77 teams guessed 'PI' on Puzzle of the Day</li>
            <li>61 teams guessed 'THECAKEISALIE' on Angry Portals</li>
            <li>57 teams guessed 'URLSARENOTAPUZZLE' on Angry Portals</li>
            <li>52 teams guessed 'SEMIAUTOMATIC' on Thunk!</li>
            <li>50 teams guessed 'DOITAGAINUSINGCRYPTICCLUES' on Zero Space (and one guessed it on Very Fun Logic Puzzle)</li>
            <li>46 teams guessed 'CAKE' on Angry Portals</li>
            <li>42 teams guessed 'PLAIN' on Puzzle of the Day</li>
            <li>38 teams guessed 'BUDDYZOO' on Famous by Association</li>
            <li>36 teams guessed 'YANKOVICWHITE' on How to Best Write an Essay</li>
        </ul>
    </li>
</ul>

<h2>Credits</h2>

<p><b>Puzzle Authors</b>: Josh Alman, Abby Caron, Brian Chen, Lewis Chen, Lennart Jansson, Maxwell Johnson, Chris Jones, DD Liu, Colin Lu, Seth Mulhall, Brendan Ney, Nathan Pinsker, Jon Schneider, Rahul Sridhar, Anderson Wang, Ben Yang, Patrick Yang, Lucy Zhang</p>

<p><b>Web Developers</b>: Brian Chen, Mitchell Gu, Lennart Jansson, Chris Jones</p>

<p><b>Additional Test Solvers and Fact Checkers</b>: Phillip Ai, Danny Bulmash, Lilly Chin, Katie Dunn, Alex Irpan, Damien Jiang, Max Murin, Jakob Weisblat, Patrick Xia, Dai Yang, Jingyi Zhao</p>

</div>



        </div>
        <div class="footer">&nbsp;</div>
        
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92192633-1', 'auto');
  ga('send', 'pageview');

</script>

        
    </body>
</html>
